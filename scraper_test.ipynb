{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00bcbef4-e7e9-4011-acb9-9b0f0f787ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "from lxml import etree\n",
    "import requests\n",
    "import ssl\n",
    "#import datefinder\n",
    "#from pdf_parser import create_folder\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66394b2-8bcd-4494-bc15-9b381dd8c95e",
   "metadata": {},
   "source": [
    "A slightly more complicated version of the code written by Sashi (https://doi.org/10.1007/s10113-020-01677-8).\n",
    "\n",
    "Main changes:\n",
    "- We keep a list of download links (instead of preventing duplicates purely by file name)\n",
    "- Download for multiple doctypes\n",
    "- Also download linked HTML pages if PDFs are unavailable, or, if both are unavailable, the text of the page itself\n",
    "- Save more meta-data\n",
    "\n",
    "For now, we still download search results sequentially (with different search links in different folders). I don't think we truly care what document resulted from what search, so in the future, we may want to re-write this using multi-threading (probably largely I/O limited anyway), but for the number of results we're getting now, it is fine to just run this in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b7710b-e69a-4b85-afa7-ab80253a585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url, returnPage=False):\n",
    "    \"\"\"Returns html element of the url of interest\n",
    "    url: string, URL of a website\n",
    "    \"\"\"\n",
    "    web_page = requests.get(url, timeout = 30)\n",
    "    web_source = html.fromstring(web_page.content)\n",
    "    if returnPage:\n",
    "        return(web_source, web_page)\n",
    "    else:\n",
    "        return web_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bcca04b-ddbb-42b1-894b-e5db32c2daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(web_source, returnAll = False):\n",
    "    \"\"\"Returns a list of topic links from the UK policies website\n",
    "    web_source: html element, output of get_page(url)\n",
    "    \"\"\"\n",
    "    #retrieve every link on the page\n",
    "    links = map(lambda tup: tup[2],list(web_source.iterlinks()))\n",
    "    links = set(links) #Makes sure they are unique\n",
    "    #Filter - many irrelevant links on the page e.g. in header and footer\n",
    "    #Here, still leaving out links with government/consultations, government/organisations and topics\n",
    "    topic_links = [l for l in links if any(\n",
    "        re.findall(r'publications|guidance|government\\/news|government\\/statistics|government\\/case-studies|government\\/collections|\\.pdf', l, re.IGNORECASE)\n",
    "    )]\n",
    "    topic_links = [l for l in topic_links if not (l.startswith(r\"/search\")|l.endswith(\"png\"))]\n",
    "    \n",
    "    if returnAll: #Mainly for debugging/checking\n",
    "        return topic_links, links\n",
    "    else:\n",
    "        return topic_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "518c6495-f709-42d5-a8e5-943d92f1ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publish_date(web_source):\n",
    "    \"\"\"Returns publishing date of policy paper as string\"\"\"\n",
    "    top_level = web_source.find_class('app-c-published-dates')\n",
    "    # published  = top_level[0].text\n",
    "    # published = list(datefinder(published))[0]\n",
    "    if len(top_level)>0:\n",
    "        published = list(map(lambda el: list(el.itertext())[0],top_level))\n",
    "        published = ' '.join(published[0].split()[-3:])\n",
    "    else:\n",
    "        #print(\"no date could be found on page\")\n",
    "        published = \"\"\n",
    "    return published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87155c90-7d90-43c9-873c-cf3e969c7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_department(web_source):\n",
    "    \"\"\"Returns department name(s) as string\"\"\"\n",
    "    #Old style (?) pages\n",
    "    top_level = web_source.find_class('app-c-publisher-metadata__definition')\n",
    "    if len(top_level)>0:\n",
    "        department = list(map(lambda el: list(el.itertext())[2],top_level))\n",
    "        return department[0]\n",
    "    else: #New style pages\n",
    "        top_level = web_source.find_class('gem-c-metadata__definition')\n",
    "        if len(top_level)>0:\n",
    "            try:\n",
    "                department = list(map(lambda el: list(el.itertext())[0],top_level[0]))\n",
    "                return department[0]\n",
    "            except:\n",
    "                return(\"\")\n",
    "        else:\n",
    "            #print(f\"no deperatment could be found on page\")\n",
    "            return(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c4e342-f2b0-452f-9955-f50c0bf9b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_link(web_source,base_url):\n",
    "    \"\"\"Returns two lists of document links\n",
    "        First, a list of pdf links\n",
    "        Second, a list of HTML links to government pages\n",
    "    \"\"\"\n",
    "    \n",
    "    global completedLinks\n",
    "    \n",
    "    links =get_topics(web_source)\n",
    "    \n",
    "    pdf_links = [l for l in links if (l.endswith(\".pdf\") and l not in completedLinks)]\n",
    "    completedLinks.extend(pdf_links)\n",
    "    html_links = [base_url+l for l in links if l.startswith(r\"/government/publications/\")]\n",
    "    html_links = [l for l in html_links if l not in completedLinks]\n",
    "    completedLinks.extend(html_links)\n",
    "    # pdf_links = list(map(lambda tup:tup[2],filter(lambda tup: tup[2].\\\n",
    "    #                 endswith('.pdf'),list(web_source.iterlinks()))))\n",
    "    \n",
    "    return(pdf_links, html_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee322b82-99fc-411f-9731-e81dbbcb142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup as bs\n",
    "# topic_url = \"https://www.gov.uk/government/publications/intergovernmental-relations-review-annual-report-for-2022\"\n",
    "# web_source, web_page = get_page(topic_url, returnPage=True)\n",
    "# soup = bs(web_page.content)\n",
    "# for c in soup.find_all(\"section\", {\"class\": \"attachment embedded\"}):\n",
    "#     if c.find(\"ti \n",
    "#     #print(c.find(\"span\", {\"class\": \"page-length\"}).text)\n",
    "#     print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c03882f9-69da-4a9f-b307-4f1256941876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_page(web_source):\n",
    "    try:\n",
    "        text = web_source.find_class(\"govuk-pagination__link-label\")[0].text\n",
    "        return(int(text.split(\"of \")[-1]))\n",
    "    except:\n",
    "        print(\"No pagination found - assuming only one page\")\n",
    "        return(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0622b52-fd13-4eff-aa7e-af24b3bacade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_save_document(full_path,document_link):\n",
    "    \"\"\"Downloads and writes PDF document to a file\"\"\"\n",
    "    file = requests.get(document_link,verify = False, timeout=30)\n",
    "    with open(full_path,'wb') as pdf:\n",
    "        pdf.write(file.content)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Saving pdf failed for {document_link} -- {e}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d43dd187-7749-4675-a437-22f26b4d4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_name_from_link(link, target_dir, suffix=\".pdf\"):\n",
    "    document_name = link.split('/')[-1]\n",
    "    document_name = document_name.split('?')[-1]\n",
    "    if not document_name.endswith(suffix):\n",
    "        document_name = document_name+suffix\n",
    "    if len(document_name) > 140:\n",
    "        document_name = document_name[:140]+suffix\n",
    "    full_path = os.path.join(target_dir,document_name) \n",
    "    if os.path.exists(full_path) == False:\n",
    "        return(full_path)\n",
    "    else: #if it already exists, add a number at the beginning\n",
    "        n = 0\n",
    "        while os.path.exists(full_path):\n",
    "            full_path = os.path.join(target_dir,f\"{n}_{document_name}\")\n",
    "            n+=1\n",
    "            if n>99:\n",
    "                print(f\"Cannot find new file location for {full_path}\\nOverwriting!\")\n",
    "                break\n",
    "        return(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669eb90b-6a80-485b-acf5-245e5ff744ce",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Copied directly from old code, but why not use pickle/JSON or csv through pandas?\n",
    "\n",
    "def read_metadata(filename):\n",
    "    \"\"\"Read existing metadata dictionary in memory\"\"\"\n",
    "    if os.path.isfile(filename):\n",
    "        metadata = eval(open(filename,'r').read())\n",
    "    else:\n",
    "        print('Creating new metadata')\n",
    "        metadata = {}\n",
    "    return metadata\n",
    "\n",
    "def add_metadata(document_name,publish_date,department, doctype, found_on, downloaded_as):\n",
    "    \"\"\"Adds metadata to dictionary if it does not already exist\"\"\"\n",
    "    try:\n",
    "        metadata[document_name]\n",
    "        print(f\"meta data for {document_name} already included -- seems you still have duplicates\")\n",
    "    except KeyError:\n",
    "        metadata[document_name] = (publish_date,department, doctype, found_on, downloaded_as)\n",
    "    return metadata\n",
    "\n",
    "def write_metadata(complete_metadata):\n",
    "    \"\"\"Writes complete collection of scraped metadata for the files\"\"\"\n",
    "    file = open('metadata.txt','w')\n",
    "    file.write(str(complete_metadata))\n",
    "    file.close()\n",
    "    return complete_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c22a1484-a154-4880-ac1a-29d095f87eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parser(target_dir, search_url, base_url,doctype,metadata, maxPage=\"auto\", waitTime=False):\n",
    "    \"\"\"Mainloop. Downloads all documents in allowed_doctypes\"\"\"\n",
    "    \n",
    "    global completedLinks\n",
    "    \n",
    "    #start timer\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n-----------\\nStarting {doctype}\\n-----------\")\n",
    "    #Counter for eligable documents\n",
    "    doc_count = 0\n",
    "    ht_count = 0\n",
    "    pg_count = 0\n",
    "    failed_count = 0\n",
    "\n",
    "    #define holder for all the metadata\n",
    "    complete_metadata = {}\n",
    "    topic_links = [None]\n",
    "    page = 0\n",
    "    if maxPage == \"auto\":\n",
    "        if doctype == \"all\":\n",
    "            maxPage = get_max_page(get_page(search_url))\n",
    "        else:\n",
    "            maxPage = get_max_page(get_page(f\"{search_url}&content_purpose_supergroup[]={doctype}\"))\n",
    "    #loop through all pages for a doctype\n",
    "    while page < maxPage:\n",
    "        page += 1\n",
    "        #Regular updates\n",
    "        if page%10 == 0:\n",
    "            print(f'Now at page {page} of {maxPage} after {int(time.time()-start_time)} seconds. \\nDownloaded: {doc_count + pg_count + ht_count} Failed: {failed_count}')\n",
    "        #web_url = \"\"\"https://www.gov.uk/government/publications?departments%5B%5D=all&from_date=&keywords=&official_document_status=all&page={}&publication_filter_option={}&subtaxons%5B%5D=all&taxons%5B%5D=all&to_date=&world_locations%5B%5D=all\"\"\".format(page,doctype)\n",
    "        if doctype == \"all\":\n",
    "            web_url = f\"{search_url}&page={page}\"\n",
    "        else:\n",
    "            web_url = f\"{search_url}&content_purpose_supergroup[]={doctype}&page={page}\"\n",
    "        try:\n",
    "            web_source = get_page(web_url)\n",
    "        except:\n",
    "            failed_count += 1\n",
    "            print(f\"Could not establish connection for {web_url}\")\n",
    "            continue\n",
    "        topic_links = get_topics(web_source)\n",
    "        #loops through all the links in the search results\n",
    "        for link in topic_links:\n",
    "            if link.startswith(\"http\"):\n",
    "                topic_url = link\n",
    "            else:\n",
    "                topic_url = base_url+link\n",
    "                \n",
    "            try:\n",
    "                web_source, web_page = get_page(topic_url, returnPage=True)\n",
    "                #print(topic_url)\n",
    "            except:\n",
    "                failed_count += 1\n",
    "                print(f\"Could not establish connection for {web_url}\")\n",
    "                continue\n",
    "            \n",
    "            #Check if this has already been done\n",
    "            if topic_url in completedLinks:\n",
    "                # if topic_url != \"https://www.gov.uk/guidance/getting-the-energy-bills-support-scheme-discount\" and doctype !=\"all\":\n",
    "                #     print(f\"NB - A Search URL was included in the completed links list & will be skipped\\n{topic_url}\\nThis may be because it is included in multiple categories\")\n",
    "                continue\n",
    "            else:\n",
    "                completedLinks.append(topic_url) \n",
    "                \n",
    "            #loop through all linked documents in one topic/search result\n",
    "            #Note that get_document_link checks against completedLinks\n",
    "            pdf_links, html_links = get_document_link(web_source,base_url)\n",
    "            publish_date = get_publish_date(web_source)\n",
    "            department = get_department(web_source)\n",
    "            \n",
    "            #if there is a pdf document linked on the webpage, download it\n",
    "            if len(pdf_links) >0:\n",
    "                for plink in pdf_links:\n",
    "                    full_path = file_name_from_link(plink, target_dir, suffix=\".pdf\")\n",
    "                    try:\n",
    "                        download_save_document(full_path,plink)\n",
    "                        metadata = add_metadata(full_path,publish_date,department, doctype, topic_url, \"linked_pdf\")\n",
    "                        doc_count += 1\n",
    "                    except requests.exceptions.MissingSchema:\n",
    "                        elink = base_url+plink\n",
    "                        try:\n",
    "                            download_save_document(full_path,elink)\n",
    "                            metadata = add_metadata(full_path,publish_date,department, doctype, topic_url, \"linked_pdf\")\n",
    "                            doc_count += 1\n",
    "                        except:\n",
    "                            print(\"Missing schema error & no fixed link found; continuing\")\n",
    "                            failed_count += 1\n",
    "                            continue                                      \n",
    "                    except requests.exceptions.ConnectionError:\n",
    "                        print(\"encountered a connection error, continuing\")\n",
    "                        failed_count += 1\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"Saving pdf failed for {plink} -- {e}\")\n",
    "                        failed_count += 1\n",
    "                        continue\n",
    "                        \n",
    "            #If there are no PDFs but there are HTML links, download those (text only)\n",
    "            elif len(html_links) > 0:\n",
    "                for hlink in html_links:\n",
    "                    full_path = file_name_from_link(hlink, target_dir, suffix=\".txt\")\n",
    "                    _, h_web_page = get_page(hlink, returnPage=True)\n",
    "                    try:\n",
    "                        with open(full_path, 'w', encoding='utf-8') as file:\n",
    "                            file.write(h_web_page.text)\n",
    "                        metadata = add_metadata(full_path,publish_date,department, doctype, topic_url, \"linked_html\")\n",
    "                        ht_count +=1\n",
    "                    except Exception as e:\n",
    "                        print(f\"could not save linked HTML: {hlink} - exception: {e}\")\n",
    "                        failed_count += 1\n",
    "                    \n",
    "            else: #If no pdf nor HTML link exists on the page, save the webpage itself (text only)\n",
    "                full_path = file_name_from_link(topic_url, target_dir, suffix=\".txt\")                \n",
    "                try:\n",
    "                    with open(full_path, 'w', encoding='utf-8') as file:\n",
    "                        file.write(web_page.text)\n",
    "                    metadata = add_metadata(full_path,publish_date,department, doctype, topic_url, \"page_itself\")\n",
    "                    pg_count +=1\n",
    "                except Exception as e:\n",
    "                    print(f\"could not save page: {link} - exception: {e}\")\n",
    "                    failed_count += 1\n",
    "                        \n",
    "            \n",
    "            if waitTime:\n",
    "                time.sleep(waitTime)\n",
    "                    \n",
    "    #write complete metadata to a text file\n",
    "    complete_metadata = write_metadata(metadata)\n",
    "    print('Scraping complete. Took {} seconds to retrieve {} pdf documents, {} linked html pages and {} pages themselves. Failed {}'\\\n",
    "          .format(int(time.time()-start_time),doc_count, ht_count, pg_count, failed_count))\n",
    "    return complete_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cdd2bc5-b376-44a6-9cb8-1b1083d53811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8124\n"
     ]
    }
   ],
   "source": [
    "#Load in the links form old meta-data or not\n",
    "loadOldLinks = True\n",
    "\n",
    "#define folder to store the documents \n",
    "prior_target_dirs = [\n",
    "    r'C:\\Users\\siets009\\OneDrive - Wageningen University & Research\\Policy documents UK NL\\Data\\230524_GlobalWarming',\n",
    "    r'C:\\Users\\siets009\\OneDrive - Wageningen University & Research\\Policy documents UK NL\\Data\\230524_ClimateChange'\n",
    "]\n",
    "\n",
    "completedLinks = set()\n",
    "    \n",
    "if loadOldLinks == True:\n",
    "    for prior_dir in prior_target_dirs:\n",
    "        with open(os.path.join(prior_dir, 'metadata.txt'), 'r') as f:\n",
    "            meta = eval(f.read())\n",
    "        for i in meta.values():\n",
    "            completedLinks.add(i[3])            \n",
    "            \n",
    "completedLinks = list(completedLinks)\n",
    "\n",
    "print(len(completedLinks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e158c957-1c46-49f5-95e3-7b38b6af1212",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "----------------\n",
      "\n",
      "Starting https://www.gov.uk/search/all?keywords=climate&order=updated-newest\n",
      "\n",
      "----------------\n",
      "----------------\n",
      "\n",
      "Creating new metadata\n",
      "\n",
      "-----------\n",
      "Starting guidance_and_regulation\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "#define folder to store the documents\n",
    "target_dirs = [\n",
    "    #r'C:\\Users\\ajsie\\OneDrive - Wageningen University & Research\\Policy documents UK NL\\Data\\230524_GlobalWarming',\n",
    "    #r'C:\\Users\\ajsie\\OneDrive - Wageningen University & Research\\Policy documents UK NL\\Data\\230524_ClimateChange',\n",
    "    r'C:\\Users\\siets009\\OneDrive - Wageningen University & Research\\Policy documents UK NL\\Data\\230524_Climate'\n",
    "]\n",
    "#base url and url and the search results for which we want to download all results\n",
    "base_url = 'https://www.gov.uk'\n",
    "search_urls = [\n",
    "    #'https://www.gov.uk/search/all?keywords=%22global+warming%22&order=updated-newest',\n",
    "    #'https://www.gov.uk/search/all?keywords=%22climate+change%22&order=updated-newest'\n",
    "    'https://www.gov.uk/search/all?keywords=climate&order=updated-newest' \n",
    "]\n",
    "\n",
    "#document types we are interested in\n",
    "#news and communications at the end as it sometimes links to other (non-news) docs as part of the news story\n",
    "#including 'all' at the after that ensures that we also include uncategorised page\n",
    "allowed_doctypes = ['guidance_and_regulation', 'research_and_statistics', 'policy_and_engagement','services', 'news_and_communications','all']\n",
    "\n",
    "for target_dir, search_url in zip(target_dirs, search_urls):\n",
    "    print(\"----------------\\n\"*2)\n",
    "    print(f\"Starting {search_url}\\n\")\n",
    "    print(\"----------------\\n\"*2)\n",
    "    \n",
    "    #Make directory in needed\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "    \n",
    "    #load metadata if available\n",
    "    metadata = read_metadata(os.path.join(target_dir, 'metadata.txt'))\n",
    "    \n",
    "    #start scraping process\n",
    "    for doctype in allowed_doctypes:\n",
    "        metadata = run_parser(target_dir, search_url, base_url,doctype,metadata, maxPage = \"auto\")#, waitTime=1.8)\n",
    "    #write to corresponding data folder    \n",
    "    with open(os.path.join(target_dir, \"metadata.txt\"), 'w') as f:\n",
    "        f.write(str(metadata))   \n",
    "    print(f\"COMPLETED --- nr of documents written: {len(metadata)}\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51621d44-7177-49d0-8380-0f2fb685884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a28200-1eff-458a-8e89-e2925ed81390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #to test\n",
    "# from bs4 import BeautifulSoup as bs\n",
    "# with open(os.path.join(target_dir, \"assess-the-impact-of-air-emissions-on-global-warming.txt\"), 'r', encoding='utf-8') as f:\n",
    "#     soup = bs(f)\n",
    "# print(soup.findAll(\"p\")) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
